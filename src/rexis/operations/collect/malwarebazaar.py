import json
import pathlib
from typing import Any, Dict, List, Optional, Tuple

from rexis.facade.malware_bazaar import query_malware_bazaar
from rexis.utils.utils import LOGGER


def collect_malwarebazaar_exec(
    tags: Optional[str] = None,
    fetch_limit: Optional[int] = None,
    batch: Optional[int] = None,
    hash: Optional[str] = None,
    hash_file: Optional[pathlib.Path] = None,
    output_path: pathlib.Path = pathlib.Path("malwarebazaar.json"),
) -> int:
    """
    Collect JSON entries from MalwareBazaar and store them to a file.

    Supports two modes:
      - hash / hash_file: query by one or many SHA256 hashes (amount=1 per hash)
      - tags: query by tag(s) with a fetch_limit per tag

    Each result row is standardized before saving using the schema:
      {
        "sha256_hash": <value>,
        "query_type": <"hash" or "tag">,
        "data": <original row from API>
      }

    Parameters mirror ingest_api but this function only collects and persists data.
    Returns the number of saved JSON objects (after deduplication by sha256_hash).
    """

    params = {
        "tags": tags,
        "fetch_limit": fetch_limit,
        "batch": batch,
        "hash": hash,
        "hash_file": str(hash_file) if hash_file else None,
        "output_path": str(output_path),
    }
    params_str = ", ".join(f"{k}={v}" for k, v in params.items() if v is not None)
    print(f"Starting collect_malwarebazaar_exec with params: {params_str}")

    print("Building collect tasks...")
    tasks = _build_collect_tasks(tags, fetch_limit, batch, hash, hash_file)
    if not tasks:
        return 0

    print(f"Executing {len(tasks)} collect task(s)...")
    all_rows: List[Dict[str, Any]] = []
    for mode, values in tasks:
        if mode == "hash":
            print("Collecting by HASH(es)...")
            rows = _collect_by_hash(values, batch)
            all_rows.extend(rows)
        elif mode == "tags":
            print("Collecting by TAGS...")
            rows = _collect_by_tags(values, fetch_limit or 0, batch or 0)
            all_rows.extend(rows)

    deduped = _dedupe_by_hash(all_rows)
    print(f"Collected {len(all_rows)} rows, deduplicated to {len(deduped)} unique sha256 entries.")

    saved = save_json(deduped, output_path)
    print(f"Saved {saved} entries to {output_path}")
    return saved


def _build_collect_tasks(
    tags: Optional[str],
    fetch_limit: Optional[int],
    batch: Optional[int],
    hash: Optional[str],
    hash_file: Optional[pathlib.Path],
) -> List[Tuple[str, List[str]]]:
    tasks: List[Tuple[str, List[str]]] = []

    if hash:
        LOGGER.info(f"Collecting by hash: {hash}")
        tasks.append(("hash", [hash]))

    elif hash_file:
        try:
            with open(hash_file, "r", encoding="utf-8") as f:
                hashes: List[str] = []
                for line in f:
                    s = line.strip()
                    if not s:
                        continue
                    if s.startswith("#"):
                        continue
                    hashes.append(s)
            LOGGER.info(f"Collecting hashes from file: {hash_file} ({len(hashes)} hashes)")
            tasks.append(("hash", hashes))
        except FileNotFoundError:
            LOGGER.error(f"File not found: {hash_file}")
            return []

    elif tags:
        tag_list: List[str] = [t.strip() for t in tags.split(",") if t.strip()]
        if fetch_limit is None or fetch_limit <= 0 or fetch_limit > 1000:
            LOGGER.error(
                "[ERROR] --fetch_limit must be provided or be > 0 when using --tags or be <= 1000 records"
            )
            return []
        LOGGER.info(
            f"Collecting by tags: {', '.join(tag_list)} (fetch_limit={fetch_limit}, batch={batch})"
        )
        tasks.append(("tags", tag_list))

    else:
        LOGGER.error("No valid input provided. Use --hash, --tags, or --hash_file.")
        return []

    return tasks


def _collect_by_hash(hashes: List[str], batch: Optional[int]) -> List[Dict[str, Any]]:
    print(
        f"Starting collection by hash: {len(hashes)} hash(es). Batch: {batch if batch else 'all'}"
    )
    all_rows: List[Dict[str, Any]] = []
    total_hashes = len(hashes)
    batch_size = batch if batch else total_hashes
    num_batches = (total_hashes + batch_size - 1) // batch_size
    current_batch = 1

    for idx, h in enumerate(hashes, 1):
        print(f"[Hash] Fetching from MalwareBazaar by sha256: {h} ({idx}/{total_hashes})")
        try:
            result = query_malware_bazaar(query_type="hash", query_value=h, amount=1)
        except Exception as e:
            LOGGER.error(f"Exception querying MalwareBazaar for hash {h}: {e}")
            continue

        if not result or "data" not in result:
            LOGGER.warning(f"No data returned for hash {h}")
            continue
        # Wrap each row using the standardized schema
        for row in result["data"]:
            sha = (row.get("sha256_hash") or row.get("sha256") or "").strip()
            wrapped = {
                "sha256_hash": sha,
                "query_type": "hash",
                "data": row,
            }
            all_rows.append(wrapped)

        if batch and (idx % batch == 0):
            print(f"[Hash] Completed batch {current_batch} of {num_batches} (batch size: {batch})")
            current_batch += 1

    print(f"Completed collection by hash. Total hashes processed: {total_hashes}.")
    return all_rows


def _collect_by_tags(tags: List[str], fetch_limit: int, batch: int) -> List[Dict[str, Any]]:
    print(f"Starting collection by tags: {tags} | fetch_limit={fetch_limit} | batch={batch}")
    all_rows: List[Dict[str, Any]] = []
    total_tags = len(tags)

    for i, tag in enumerate(tags, 1):
        print(
            f"[Tags] Fetching from MalwareBazaar by tag: {tag} ({i}/{total_tags}) limit={fetch_limit}"
        )
        try:
            result = query_malware_bazaar(query_type="tag", query_value=tag, amount=fetch_limit)
        except Exception as e:
            LOGGER.error(f"Exception querying MalwareBazaar for tag {tag}: {e}")
            continue
        if not result or "data" not in result:
            LOGGER.warning(f"No data returned for tag {tag}")
            continue
        # Wrap each row using the standardized schema
        for row in result["data"]:
            sha = (row.get("sha256_hash") or row.get("sha256") or "").strip()
            wrapped = {
                "sha256_hash": sha,
                "query_type": "tag",
                "data": row,
            }
            all_rows.append(wrapped)

    print(f"Completed collection by tags. Tags processed: {total_tags}.")
    return all_rows


def _dedupe_by_hash(rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    seen: set[str] = set()
    out: List[Dict[str, Any]] = []
    for r in rows:
        sha = (r.get("sha256_hash") or "").strip()
        if not sha:
            # keep entries even if sha is missing, but log once per item
            LOGGER.debug("Entry missing sha256_hash field; keeping as-is.")
            out.append(r)
            continue
        if sha in seen:
            continue
        seen.add(sha)
        out.append(r)
    LOGGER.info(f"Deduplicated to {len(out)} unique rows (by sha256_hash).")
    return out


def save_json(rows: List[Dict[str, Any]], path: pathlib.Path) -> int:
    try:
        path.parent.mkdir(parents=True, exist_ok=True)
        path.write_text(json.dumps(rows, ensure_ascii=False, indent=2) + "\n", encoding="utf-8")
        LOGGER.info(f"Successfully saved {len(rows)} MalwareBazaar rows to {path}")
        return len(rows)
    except Exception as e:
        LOGGER.error(f"Failed to save JSON to {path}: {e}")
        raise
