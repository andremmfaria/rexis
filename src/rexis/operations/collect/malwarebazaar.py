import json
import pathlib
import time
from typing import Any, Dict, List, Optional, Set, Tuple

from rexis.tools.malware_bazaar import query_malware_bazaar
from rexis.utils.utils import LOGGER, get_version


def collect_malwarebazaar_exec(
    tags: Optional[str] = None,
    fetch_limit: Optional[int] = None,
    batch: Optional[int] = None,
    hash: Optional[str] = None,
    hash_file: Optional[pathlib.Path] = None,
    output_path: pathlib.Path = pathlib.Path("malwarebazaar.json"),
) -> int:
    """
    Collect JSON entries from MalwareBazaar and store them to a file.

    Supports two modes:
      - hash / hash_file: query by one or many SHA256 hashes (amount=1 per hash)
      - tags: query by tag(s) with a fetch_limit per tag

    Each result row is standardized before saving using the schema:
      {
        "sha256_hash": <value>,
        "query_type": <"hash" or "tag">,
        "data": <original row from API>
      }

    Parameters mirror ingest_api but this function only collects and persists data.

    Returns:
        int: Number of saved JSON objects (after deduplication by sha256_hash).
    """

    start_ts = time.time()
    started_at = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime(start_ts))

    base_path: str = output_path.stem
    run_dir: pathlib.Path = output_path.parent / base_path
    run_dir.mkdir(parents=True, exist_ok=True)

    params: Dict[str, Any] = {
        "tags": tags,
        "fetch_limit": fetch_limit,
        "batch": batch,
        "hash": hash,
        "hash_file": str(hash_file) if hash_file else None,
        "output_path": str(output_path),
    }
    params_str: str = ", ".join(f"{k}={v}" for k, v in params.items() if v is not None)
    print(f"[collect-mb] Starting collect_malwarebazaar_exec with params: {params_str}")

    status: str = "success"
    error_message: Optional[str] = None
    metrics: Dict[str, Any] = {
        "tasks": None,
        "rows_collected": None,
        "rows_deduped": None,
        "saved_entries": None,
    }
    saved: int = 0

    try:
        print("[collect-mb] Building collect tasks...")
        collect_tasks: List[Tuple[str, List[str]]] = _build_collect_tasks(
            tags, fetch_limit, batch, hash, hash_file
        )
        metrics["tasks"] = len(collect_tasks)
        if not collect_tasks:
            print("[collect-mb] No tasks to run.")
            return 0

        print(f"[collect-mb] Executing {len(collect_tasks)} collect task(s)...")
        all_rows: List[Dict[str, Any]] = []
        for mode, values in collect_tasks:
            if mode == "hash":
                print("[collect-mb] Collecting by HASH(es)...")
                hash_rows: List[Dict[str, Any]] = _collect_by_hash(values, batch)
                all_rows.extend(hash_rows)
            elif mode == "tags":
                print("[collect-mb] Collecting by TAGS...")
                tag_rows: List[Dict[str, Any]] = _collect_by_tags(
                    values, fetch_limit or 0, batch or 0
                )
                all_rows.extend(tag_rows)

        metrics["rows_collected"] = len(all_rows)
        deduped: List[Dict[str, Any]] = _dedupe_by_hash(all_rows)
        metrics["rows_deduped"] = len(deduped)
        print(
            f"[collect-mb] Collected {len(all_rows)} rows, deduplicated to {len(deduped)} unique sha256 entries."
        )

        saved = save_json(deduped, output_path)
        metrics["saved_entries"] = saved
        print(f"[collect-mb] Saved {saved} entries to {output_path}")
    except Exception as e:
        LOGGER.error("MalwareBazaar collection failed: %s", e)
        status = "error"
        error_message = str(e)
        exc = e
    else:
        exc = None
    finally:
        end_ts = time.time()
        ended_at = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime(end_ts))
        duration_sec = round(end_ts - start_ts, 3)
        print(f"[collect-mb] Preparing run report (status={status}, duration={duration_sec}s)...")
        report: Dict[str, Any] = {
            "run_id": base_path,
            "base_path": base_path,
            "started_at": started_at,
            "ended_at": ended_at,
            "duration_seconds": duration_sec,
            "status": status,
            "error": error_message,
            "summary": metrics,
            "inputs": {
                "tags": tags,
                "fetch_limit": fetch_limit,
                "batch": batch,
                "hash": hash,
                "hash_file": str(hash_file) if hash_file else None,
            },
            "outputs": {
                "output_path": str(output_path),
                "run_dir": str(run_dir),
            },
            "environment": {
                "rexis_version": get_version(),
            },
        }
        report_path: pathlib.Path = run_dir / f"{base_path}.report.json"
        try:
            report_path.write_text(json.dumps(report, indent=2) + "\n", encoding="utf-8")
            LOGGER.info(f"Run report written to {report_path}")
        except Exception as rexc:
            LOGGER.error("Failed to write run report %s: %s", report_path, rexc)

    if exc:
        raise exc

    return saved


def _build_collect_tasks(
    tags: Optional[str],
    fetch_limit: Optional[int],
    batch: Optional[int],
    hash: Optional[str],
    hash_file: Optional[pathlib.Path],
) -> List[Tuple[str, List[str]]]:
    """Build the list of collection tasks based on user inputs.

    Args:
        tags: Comma-separated tags to query.
        fetch_limit: Maximum number of items per tag.
        batch: Batch size for progress output when using hash mode.
        hash: Single SHA256 hash to query.
        hash_file: File containing one SHA256 hash per line (comments with '#').

    Returns:
        List[Tuple[str, List[str]]]: A list of tasks represented as (mode, values). Mode is
        either "hash" with a list of sha256 strings, or "tags" with a list of tag strings.
    """
    collect_tasks: List[Tuple[str, List[str]]] = []

    if hash:
        LOGGER.info(f"Collecting by hash: {hash}")
        collect_tasks.append(("hash", [hash]))

    elif hash_file:
        try:
            with open(hash_file, "r", encoding="utf-8") as file_handle:
                sha_list: List[str] = []
                for line in file_handle:
                    line_str: str = line.strip()
                    if not line_str or line_str.startswith("#"):
                        continue
                    sha_list.append(line_str)
            LOGGER.info(f"Collecting hashes from file: {hash_file} ({len(sha_list)} hashes)")
            collect_tasks.append(("hash", sha_list))
        except FileNotFoundError:
            LOGGER.error(f"File not found: {hash_file}")
            return []

    elif tags:
        tag_list: List[str] = [t.strip() for t in tags.split(",") if t.strip()]
        if fetch_limit is None or fetch_limit <= 0 or fetch_limit > 1000:
            LOGGER.error(
                "[ERROR] --fetch_limit must be provided or be > 0 when using --tags or be <= 1000 records"
            )
            return []
        LOGGER.info(
            f"Collecting by tags: {', '.join(tag_list)} (fetch_limit={fetch_limit}, batch={batch})"
        )
        collect_tasks.append(("tags", tag_list))

    else:
        LOGGER.error("No valid input provided. Use --hash, --tags, or --hash_file.")
        return []

    return collect_tasks


def _collect_by_hash(hashes: List[str], batch: Optional[int]) -> List[Dict[str, Any]]:
    print(
        f"[collect-mb] Starting collection by hash: {len(hashes)} hash(es). Batch: {batch if batch else 'all'}"
    )
    all_rows: List[Dict[str, Any]] = []
    total_hashes: int = len(hashes)
    batch_size: int = batch if batch else total_hashes
    num_batches: int = (total_hashes + batch_size - 1) // batch_size
    current_batch: int = 1

    for idx, sha_value in enumerate(hashes, 1):
        print(
            f"[collect-mb] Fetching from MalwareBazaar by sha256: {sha_value} ({idx}/{total_hashes})"
        )
        try:
            result: Dict[str, Any] = query_malware_bazaar(
                query_type="hash", query_value=sha_value, amount=1
            )
        except Exception as e:
            LOGGER.error(f"Exception querying MalwareBazaar for hash {sha_value}: {e}")
            continue

        if not result or "data" not in result:
            LOGGER.warning(f"No data returned for hash {sha_value}")
            continue
        # Wrap each row using the standardized schema
        for row in result["data"]:
            sha256: str = (row.get("sha256_hash") or row.get("sha256") or "").strip()
            wrapped: Dict[str, Any] = {
                "sha256_hash": sha256,
                "query_type": "hash",
                "data": row,
            }
            all_rows.append(wrapped)

        if batch and (idx % batch == 0):
            print(
                f"[collect-mb] Completed batch {current_batch} of {num_batches} (batch size: {batch})"
            )
            current_batch += 1

    print(f"[collect-mb] Completed collection by hash. Total hashes processed: {total_hashes}.")
    return all_rows


def _collect_by_tags(tags: List[str], fetch_limit: int, batch: int) -> List[Dict[str, Any]]:
    print(
        f"[collect-mb] Starting collection by tags: {tags} | fetch_limit={fetch_limit} | batch={batch}"
    )
    all_rows: List[Dict[str, Any]] = []
    total_tags: int = len(tags)

    for index, tag in enumerate(tags, 1):
        print(
            f"[collect-mb] Fetching from MalwareBazaar by tag: {tag} ({index}/{total_tags}) limit={fetch_limit}"
        )
        try:
            result: Dict[str, Any] = query_malware_bazaar(
                query_type="tag", query_value=tag, amount=fetch_limit
            )
        except Exception as e:
            LOGGER.error(f"Exception querying MalwareBazaar for tag {tag}: {e}")
            continue
        if not result or "data" not in result:
            LOGGER.warning(f"No data returned for tag {tag}")
            continue
        # Wrap each row using the standardized schema
        for row in result["data"]:
            sha256: str = (row.get("sha256_hash") or row.get("sha256") or "").strip()
            wrapped: Dict[str, Any] = {
                "sha256_hash": sha256,
                "query_type": "tag",
                "data": row,
            }
            all_rows.append(wrapped)

    print(f"[collect-mb] Completed collection by tags. Tags processed: {total_tags}.")
    return all_rows


def _dedupe_by_hash(rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Deduplicate rows by their 'sha256_hash' field.

    Args:
        rows: List of standardized result rows.

    Returns:
        List[Dict[str, Any]]: Deduplicated list preserving order.
    """
    seen: Set[str] = set()
    deduplicated_rows: List[Dict[str, Any]] = []
    for row in rows:
        sha256: str = (row.get("sha256_hash") or "").strip()
        if not sha256:
            # keep entries even if sha is missing, but log once per item
            LOGGER.debug("Entry missing sha256_hash field; keeping as-is.")
            deduplicated_rows.append(row)
            continue
        if sha256 in seen:
            continue
        seen.add(sha256)
        deduplicated_rows.append(row)
    LOGGER.info(f"Deduplicated to {len(deduplicated_rows)} unique rows (by sha256_hash).")
    return deduplicated_rows


def save_json(rows: List[Dict[str, Any]], path: pathlib.Path) -> int:
    """Persist rows as pretty-printed JSON to the given path.

    Args:
        rows: List of standardized result rows to save.
        path: Output file path.

    Returns:
        int: Number of rows written.
    """
    try:
        path.parent.mkdir(parents=True, exist_ok=True)
        path.write_text(json.dumps(rows, ensure_ascii=False, indent=2) + "\n", encoding="utf-8")
        LOGGER.info(f"Successfully saved {len(rows)} MalwareBazaar rows to {path}")
        return len(rows)
    except Exception as e:
        LOGGER.error(f"Failed to save JSON to {path}: {e}")
        raise
