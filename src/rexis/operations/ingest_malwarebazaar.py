import json
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
from haystack.components.embedders import OpenAIDocumentEmbedder
from haystack.components.preprocessors import DocumentSplitter
from haystack.components.writers import DocumentWriter
from haystack.dataclasses import Document
from haystack.document_stores.types import DuplicatePolicy
from haystack.utils import Secret
from haystack_integrations.document_stores.pgvector import PgvectorDocumentStore
from rexis.facade.malware_bazaar import query_malware_bazaar
from rexis.utils.config import config
from rexis.utils.constants import DATABASE_CONNECTION_CONNSTRING
from rexis.utils.utils import LOGGER


def ingest_malwarebazaar_exec(
    tags: Optional[str] = None,
    fetch_limit: Optional[int] = None,
    batch: Optional[int] = None,
    hash: Optional[str] = None,
    hash_file: Optional[Path] = None,
) -> None:
    tasks = _build_ingest_tasks(tags, fetch_limit, batch, hash, hash_file)
    if not tasks:
        return

    for mode, values in tasks:
        if mode == "hash":
            _ingest_by_hash(values, batch)
        elif mode == "tags":
            _ingest_by_tags(values, fetch_limit, batch)


def index_documents(documents: List[Document], refresh: bool = True) -> None:
    if not documents:
        LOGGER.warning("No documents provided for indexing.")
        return

    LOGGER.info("Starting indexing for %d documents...", len(documents))

    prepped_docs: List[Document] = _prepare_documents_for_indexing(documents)
    chunked_docs: List[Document] = _split_documents(prepped_docs)
    embedded_chunks: List[Document] = _embed_chunks(chunked_docs)
    rejoined_documents: List[Document] = _rejoin_embedded_chunks(embedded_chunks)
    _write_documents_to_db(rejoined_documents, refresh=refresh)
    LOGGER.info("Indexing complete.")


def _build_ingest_tasks(
    tags: Optional[str],
    fetch_limit: Optional[int],
    batch: Optional[int],
    hash: Optional[str],
    hash_file: Optional[Path],
) -> List[Tuple[str, List[str]]]:
    tasks: List[Tuple[str, List[str]]] = []

    if hash:
        LOGGER.info(f"Ingesting by hash: {hash}")
        tasks.append(("hash", [hash]))

    elif hash_file:
        try:
            with open(hash_file, "r", encoding="utf-8") as f:
                hashes = [line.strip() for line in f if line.strip()]
            LOGGER.info(f"Ingesting hashes from file: {hash_file} ({len(hashes)} hashes)")
            tasks.append(("hash", hashes))
        except FileNotFoundError:
            LOGGER.error(f"File not found: {hash_file}")
            return []

    elif tags:
        tag_list: List[str] = [t.strip() for t in tags.split(",") if t.strip()]
        if fetch_limit is None or fetch_limit <= 0:
            LOGGER.error("--fetch_limit must be provided and > 0 when using --tags.")
            return []
        if batch is None or batch <= 0:
            LOGGER.error("--batch must be provided and > 0 when using --tags.")
            return []
        LOGGER.info(
            f"Ingesting by tags: {', '.join(tag_list)} (fetch_limit={fetch_limit}, batch={batch})"
        )
        tasks.append(("tags", tag_list))

    else:
        LOGGER.error("No valid input provided. Use --hash, --tags, or --hash_file.")
        return []

    return tasks


def _process_and_index(documents_batch):
    documents_batch: List[Document]
    if documents_batch:
        LOGGER.info(f"Indexing {len(documents_batch)} documents...")
        index_documents(documents=documents_batch)
        LOGGER.info("Batch indexing completed.")


def _ingest_by_hash(hashes: List[str], batch: Optional[int]):
    # Log the start of the ingestion process, including the number of hashes and batch size
    LOGGER.info(
        f"Starting ingestion by hash for {len(hashes)} hashes. Batch size: {batch if batch else 'all'}."
    )
    all_documents: List[Document] = []
    total_hashes = len(hashes)
    batch_size = batch if batch else total_hashes
    num_batches = (total_hashes + batch_size - 1) // batch_size
    current_batch = 1

    # Iterate over each hash, fetching and processing documents
    for idx, h in enumerate(hashes, 1):
        LOGGER.info(f"[Hash] Fetching from MalwareBazaar by hash-256: {h} ({idx}/{total_hashes})")
        try:
            # Query MalwareBazaar for the current hash
            result = query_malware_bazaar(query_type="hash", query_value=h, amount=1)
        except Exception as e:
            LOGGER.error(f"Exception querying MalwareBazaar for hash {h}: {e}")
            continue
        if not result or "data" not in result:
            LOGGER.warning(f"No data returned for hash {h}")
            continue
        # Wrap the API result into Document objects
        docs = _wrap_malwarebazaar_documents(result["data"])
        all_documents.extend(docs)

        # If the batch size is reached, process and index the batch
        if batch and len(all_documents) >= batch:
            LOGGER.info(
                f"[Hash] Processing batch {current_batch} of {num_batches} (batch size: {batch})"
            )
            _process_and_index(all_documents)
            all_documents = []
            current_batch += 1

    # Process any remaining documents that didn't fill a complete batch
    if all_documents:
        LOGGER.info(
            f"[Hash] Processing batch {current_batch} of {num_batches} (batch size: {len(all_documents)})"
        )
        _process_and_index(all_documents)

    LOGGER.info(f"Completed ingestion by hash. Total hashes processed: {total_hashes}.")


def _ingest_by_tags(tags: List[str], fetch_limit: int, batch: int):
    # Log the start of the ingestion process with the provided tags, fetch limit, and batch size
    LOGGER.info(f"Starting ingestion by tags: {tags} | fetch_limit={fetch_limit} | batch={batch}")
    all_documents: List[Document] = []
    docs_per_tag: List[int] = []

    # First, query MalwareBazaar for each tag to count the number of documents per tag
    for tag in tags:
        try:
            result = query_malware_bazaar(query_type="tag", query_value=tag, amount=fetch_limit)
        except Exception as e:
            LOGGER.error(f"Exception querying MalwareBazaar for tag {tag}: {e}")
            continue
        if not result or "data" not in result:
            LOGGER.warning(f"No data returned for tag {tag}")
            continue
        docs_per_tag.append(len(result["data"]))

    # Calculate the total number of documents to be ingested across all tags
    grand_total_docs = sum(docs_per_tag)
    if grand_total_docs == 0:
        LOGGER.warning("No documents found for any tag.")
        return

    # Determine the number of batches needed
    num_batches = (grand_total_docs + batch - 1) // batch
    current_batch = 1
    doc_counter = 0

    # For each tag, fetch the documents and process them in batches
    for tag in tags:
        LOGGER.info(f"[Tags] Fetching from MalwareBazaar by tag: {tag} (limit={fetch_limit})")
        try:
            result = query_malware_bazaar(query_type="tag", query_value=tag, amount=fetch_limit)
        except Exception as e:
            LOGGER.error(f"Exception querying MalwareBazaar for tag {tag}: {e}")
            continue
        if not result or "data" not in result:
            LOGGER.warning(f"No data returned for tag {tag}")
            continue

        # Wrap the API results into Document objects
        docs = _wrap_malwarebazaar_documents(result["data"])
        for doc in docs:
            all_documents.append(doc)
            doc_counter += 1
            # If the batch size is reached, process and index the batch
            if len(all_documents) >= batch:
                LOGGER.info(
                    f"[Tags] Processing batch {current_batch} of {num_batches} (batch size: {batch}) [{doc_counter}/{grand_total_docs} docs]"
                )
                _process_and_index(all_documents)
                all_documents = []
                current_batch += 1

    # Process any remaining documents that didn't fill a complete batch
    if all_documents:
        LOGGER.info(
            f"[Tags] Processing batch {current_batch} of {num_batches} (batch size: {len(all_documents)}) [{doc_counter}/{grand_total_docs} docs]"
        )
        _process_and_index(all_documents)

    LOGGER.info(f"Completed ingestion by tags. Total docs processed: {grand_total_docs}.")


def _prepare_documents_for_indexing(documents: List[Document]) -> List[Document]:
    prepped_docs: List[Document] = []
    for doc in documents:
        if not isinstance(doc.content, str):
            try:
                doc.content = json.dumps(doc.content)
            except Exception:
                doc.content = str(doc.content)
        meta = dict(doc.meta or {})
        meta.setdefault("parent_id", doc.id)
        prepped_docs.append(Document(id=doc.id, content=doc.content, meta=meta))
    return prepped_docs


def _split_documents(documents: List[Document]) -> List[Document]:
    # Create a DocumentSplitter instance to split documents by words,
    # with each chunk containing up to 400 words and 50 words overlap between chunks.
    splitter: DocumentSplitter = DocumentSplitter(
        split_by="word",
        split_length=400,
        split_overlap=50,
        respect_sentence_boundary=False,
    )

    LOGGER.info("Splitting documents into chunks (word-based, length=400, overlap=50)...")

    # Run the splitter on the input documents. This returns a dict with a "documents" key.
    split_result = splitter.run(documents=documents)

    # Extract the list of split (chunked) documents from the result.
    chunked_docs: List[Document] = split_result.get("documents", [])

    LOGGER.info(
        "Chunking complete. %d chunks produced from %d input documents.",
        len(chunked_docs),
        len(documents),
    )

    # Group the chunked documents by their parent document ID.
    by_parent: Dict[str, List[Document]] = {}
    for d in chunked_docs:
        parent_id = d.meta.get("parent_id") or d.id
        if parent_id not in by_parent:
            by_parent[parent_id] = []
        by_parent[parent_id].append(d)

    # For each group of chunks belonging to the same parent document:
    for parent_id, chunks in by_parent.items():
        total = len(chunks)
        # Assign chunk index and total chunk count metadata, and set a unique chunk ID.
        for idx, ch in enumerate(chunks):
            ch.meta["chunk_index"] = idx
            ch.meta["total_chunks"] = total
            ch.id = f"{parent_id}::chunk-{idx}"

    # Return the list of all chunked documents.
    return chunked_docs


def _embed_chunks(chunked_docs: List[Document]) -> List[Document]:
    embedder: OpenAIDocumentEmbedder = OpenAIDocumentEmbedder(
        api_key=Secret.from_token(config.models.openai.api_key),
        model=config.models.openai.embedding_model,
    )
    LOGGER.info("Embedding %d chunked documents...", len(chunked_docs))
    emb_result = embedder.run(documents=chunked_docs)
    embedded_chunks: List[Document] = emb_result["documents"]
    LOGGER.info("Embedding complete. %d chunks embedded.", len(embedded_chunks))
    return embedded_chunks


def _rejoin_embedded_chunks(embedded_chunks: List[Document]) -> List[Document]:
    # Group chunks by their parent_id (or their own id if parent_id is missing)
    grouped_chunks: Dict[str, List[Document]] = {}
    for ch in embedded_chunks:
        parent_id: str = ch.meta.get("parent_id", ch.id)
        # Initialize the list if this parent_id hasn't been seen yet
        if parent_id not in grouped_chunks:
            grouped_chunks[parent_id] = []
        grouped_chunks[parent_id].append(ch)

    LOGGER.info("Rejoining embedded chunks back to parent documents...")
    rejoined_documents: List[Document] = []
    for parent_id, chunks in grouped_chunks.items():
        # If there's only one chunk and it's not a split chunk, just keep it as is
        if len(chunks) == 1 and "::chunk-" not in chunks[0].id:
            rejoined_documents.extend(chunks)
            continue

        # Collect all available embeddings for the chunks
        vecs: List[np.ndarray] = [
            np.array(ch.embedding, dtype=np.float32) for ch in chunks if ch.embedding is not None
        ]
        if not vecs:
            # If no embeddings are found, skip this group
            LOGGER.warning("No embeddings found for parent_id=%s; skipping.", parent_id)
            continue
        # Average the embeddings to create a single embedding for the rejoined document
        avg_embedding: List[float] = np.mean(vecs, axis=0).tolist()
        # Combine the content of all chunks, sorted by their chunk_index
        combined_content: str = "\n\n".join(
            ch.content for ch in sorted(chunks, key=lambda x: x.meta.get("chunk_index", 0))
        )
        # Copy the metadata from the first chunk, removing chunk-specific keys
        base_meta: Dict[str, Any] = dict(chunks[0].meta or {})
        base_meta.pop("chunk_index", None)
        base_meta.pop("total_chunks", None)
        # Create a new Document representing the rejoined parent document
        rejoined_documents.append(
            Document(
                id=parent_id,
                content=combined_content,
                embedding=avg_embedding,
                meta=base_meta,
            )
        )
    LOGGER.info("Rejoin complete. %d parent documents ready to write.", len(rejoined_documents))
    return rejoined_documents


def _write_documents_to_db(documents: List[Document], refresh: bool = True) -> None:
    LOGGER.info("Connecting to PgvectorDocumentStore...")
    doc_store: PgvectorDocumentStore = PgvectorDocumentStore(
        connection_string=Secret.from_token(DATABASE_CONNECTION_CONNSTRING),
        embedding_dimension=1536,
        vector_function="cosine_similarity",
        recreate_table=False,
        search_strategy="hnsw",
        hnsw_recreate_index_if_exists=True,
    )
    writer: DocumentWriter = DocumentWriter(
        document_store=doc_store,
        policy=DuplicatePolicy.OVERWRITE if refresh else DuplicatePolicy.SKIP,
    )
    LOGGER.info("Writing %d documents to database (refresh=%s)...", len(documents), refresh)
    writer.run(documents=documents)


def _wrap_malwarebazaar_documents(api_data: List[dict]) -> List[Document]:
    """
    Transforms a List of MalwareBazaar API result dictionaries into a List of Haystack Document objects.

    Each API result is serialized to JSON and wrapped in a Document, with the SHA256 hash used as part of the document ID
    and included in the metadata. Also adds tags, timestamps, and imported time to the metadata.

    Args:
        api_data (List[dict]): A List of dictionaries, each representing a sample returned by the MalwareBazaar API.

    Returns:
        List[Document]: A List of Haystack Document objects, each containing the serialized sample data and metadata.

    Logs:
        - The number of API results being wrapped.
        - The SHA256 hash for each sample as it is processed.
        - The total number of documents created.
    """

    LOGGER.debug(f"Wrapping {len(api_data)} MalwareBazaar API results into Document objects.")
    documents: List[Document] = []
    imported_time: str = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
    sample: dict
    for sample in api_data:
        hash_val: str = sample.get("sha256_hash", "")
        content: str = json.dumps(sample)
        tags: List = sample.get("tags", [])
        # Try to get timestamp fields, fallback to None if not present
        first_seen = sample.get("first_seen", None)
        last_seen = sample.get("last_seen", None)
        LOGGER.debug(f"Creating Document for sha256: {hash_val}")
        documents.append(
            Document(
                id=hash_val,
                content=content,
                meta={
                    "sha256": hash_val,
                    "tags": tags,
                    "first_seen": first_seen,
                    "last_seen": last_seen,
                    "imported_time": imported_time,
                },
            )
        )
    LOGGER.info(f"Wrapped {len(documents)} documents from MalwareBazaar API data.")
    return documents
